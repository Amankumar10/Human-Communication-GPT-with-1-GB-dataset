{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948ec721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Loading dataset: RoskoN/dailydialog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "c:\\Users\\aman\\Desktop\\Minimal GPT-2 architecture (PyTorch-only)\\env\\lib\\site-packages\\huggingface_hub\\file_download.py:120: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\aman\\.cache\\huggingface\\hub\\datasets--roskoN--dailydialog. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Generating train split: 11118 examples [00:00, 18977.21 examples/s]\n",
      "Generating validation split: 1000 examples [00:00, 15632.13 examples/s]\n",
      "Generating test split: 1000 examples [00:00, 15906.37 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ Loading dataset: KoTfi/empathetic_dialogues_parquet\n",
      "‚ö†Ô∏è  Could not load KoTfi/empathetic_dialogues_parquet: Dataset 'KoTfi/empathetic_dialogues_parquet' doesn't exist on the Hub or cannot be accessed.\n",
      "üîÅ Trying alternate dataset: empathetic_dialogues\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28.0M/28.0M [00:06<00:00, 4.53MB/s]\n",
      "Generating train split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 76673/76673 [00:03<00:00, 22042.36 examples/s]\n",
      "Generating validation split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12030/12030 [00:01<00:00, 10572.44 examples/s]\n",
      "Generating test split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10943/10943 [00:00<00:00, 11779.14 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processing DailyDialog data...\n",
      "‚úÖ Processing EmpatheticDialogues data...\n",
      "üß© Combining 87791 dialogues...\n",
      "‚úÖ Dataset saved to data/human_chat.txt\n",
      "üì¶ Total characters: 12,950,920\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "prepare_dataset.py\n",
    "-------------------\n",
    "Downloads and merges DailyDialog + EmpatheticDialogues datasets\n",
    "into a single human_chat.txt file for training Human Communication GPT.\n",
    "Now supports Hugging Face's new `trust_remote_code=True` requirement.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "def safe_load_dataset(name, alt_name=None):\n",
    "    \"\"\"Try to load dataset with trust_remote_code=True, fallback if needed.\"\"\"\n",
    "    try:\n",
    "        print(f\"üîπ Loading dataset: {name}\")\n",
    "        return load_dataset(name, trust_remote_code=True)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Could not load {name}: {e}\")\n",
    "        if alt_name:\n",
    "            print(f\"üîÅ Trying alternate dataset: {alt_name}\")\n",
    "            try:\n",
    "                return load_dataset(alt_name, trust_remote_code=True)\n",
    "            except Exception as e2:\n",
    "                print(f\"‚ö†Ô∏è  Alternate failed too: {e2}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def main():\n",
    "    daily = safe_load_dataset(\"RoskoN/dailydialog\", alt_name=\"daily_dialog\")\n",
    "    empathetic = safe_load_dataset(\"KoTfi/empathetic_dialogues_parquet\", alt_name=\"empathetic_dialogues\")\n",
    "\n",
    "    dialogs = []\n",
    "\n",
    "    if daily:\n",
    "        print(\"‚úÖ Processing DailyDialog data...\")\n",
    "        split_name = \"train\" if \"train\" in daily else list(daily.keys())[0]\n",
    "        for d in daily[split_name]:\n",
    "            if isinstance(d, dict):\n",
    "                if \"dialog\" in d:\n",
    "                    dialogs.append(\"\\n\".join(d[\"dialog\"]))\n",
    "                elif \"utterances\" in d:\n",
    "                    dialogs.append(\"\\n\".join(d[\"utterances\"]))\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No DailyDialog data loaded.\")\n",
    "\n",
    "    if empathetic:\n",
    "        print(\"‚úÖ Processing EmpatheticDialogues data...\")\n",
    "        split_name = \"train\" if \"train\" in empathetic else list(empathetic.keys())[0]\n",
    "        for d in empathetic[split_name]:\n",
    "            if isinstance(d, dict):\n",
    "                context = d.get(\"context\", \"\")\n",
    "                utter = d.get(\"utterance\", \"\")\n",
    "                dialogs.append(f\"{context}\\n{utter}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  No EmpatheticDialogues data loaded.\")\n",
    "\n",
    "    print(f\"üß© Combining {len(dialogs)} dialogues...\")\n",
    "    text = \"\\n\\n\".join(dialogs)\n",
    "\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "    with open(\"data/human_chat.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "    print(\"‚úÖ Dataset saved to data/human_chat.txt\")\n",
    "    print(f\"üì¶ Total characters: {len(text):,}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6acee5dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'RoskoN/dailydialog' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì• Loading RoskoN/dailydialog ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since RoskoN/dailydialog couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'full' at C:\\Users\\aman\\.cache\\huggingface\\datasets\\RoskoN___dailydialog\\full\\1.0.0\\7d96d5a6afcb95cf518611d5147758f4a5991bab51dc97c3a8131b6fb7811b76 (last modified on Sat Nov  1 09:40:46 2025).\n",
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'facebook/empathetic_dialogues' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipped RoskoN/dailydialog due to error:\n",
      "string indices must be integers\n",
      "\n",
      "\n",
      "üì• Loading facebook/empathetic_dialogues ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since facebook/empathetic_dialogues couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at C:\\Users\\aman\\.cache\\huggingface\\datasets\\facebook___empathetic_dialogues\\default\\0.1.0\\09bbeed3882a67db98c73952fb3c1c9a85af83dc78f81454c2454382fd03f6cf (last modified on Sun Nov  2 18:15:39 2025).\n",
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'allenai/blended_skill_talk' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ facebook/empathetic_dialogues: added 99,530 dialogues, total ~9.71 MB\n",
      "\n",
      "üì• Loading allenai/blended_skill_talk ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'bavard/personachat_truecased' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipped allenai/blended_skill_talk due to error:\n",
      "Dataset 'allenai/blended_skill_talk' doesn't exist on the Hub or cannot be accessed.\n",
      "\n",
      "\n",
      "üì• Loading bavard/personachat_truecased ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since bavard/personachat_truecased couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'full' at C:\\Users\\aman\\.cache\\huggingface\\datasets\\bavard___personachat_truecased\\full\\1.0.0\\73ee8f1a0d9e42255af5a8301877a2f3ac638e55b1cd9cbccca5ab7e23d2b638 (last modified on Sun Nov  2 18:16:30 2025).\n",
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'open_subtitles' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ bavard/personachat_truecased: added 0 dialogues, total ~9.71 MB\n",
      "\n",
      "üì• Loading open_subtitles ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since open_subtitles couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'en-hi' at C:\\Users\\aman\\.cache\\huggingface\\datasets\\open_subtitles\\en-hi\\2018.0.0\\c1ec973ca4b6e588740d8f167cc0e24ea3f626e70bc7ffe467e944730500e198 (last modified on Sun Nov  2 18:23:30 2025).\n",
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'multi_woz_v22' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ open_subtitles: added 0 dialogues, total ~9.71 MB\n",
      "\n",
      "üì• Loading multi_woz_v22 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since multi_woz_v22 couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'v2.2_active_only' at C:\\Users\\aman\\.cache\\huggingface\\datasets\\multi_woz_v22\\v2.2_active_only\\2.2.0\\6719c8b21478299411a0c6fdb7137c3ebab2e6425129af831687fb7851c69eb5 (last modified on Sun Nov  2 18:18:13 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ multi_woz_v22: added 0 dialogues, total ~9.71 MB\n",
      "\n",
      "‚úÖ Saved merged dataset: data/human_chat_1gb.txt\n",
      "üì¶ Final size: 9.91 MB (99,530 dialogues)\n",
      "\n",
      "üéâ Done! You can now train your Human Communication GPT.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import re, os\n",
    "\n",
    "# ----------------------- Config -----------------------\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "dialogs = []\n",
    "total_chars = 0\n",
    "\n",
    "datasets_to_load = [\n",
    "    (\"RoskoN/dailydialog\", None),\n",
    "    (\"facebook/empathetic_dialogues\", None),\n",
    "    (\"allenai/blended_skill_talk\", None),\n",
    "    (\"bavard/personachat_truecased\", None),\n",
    "    (\"open_subtitles\", \"en-hi\"),\n",
    "    (\"multi_woz_v22\", None),\n",
    "]\n",
    "\n",
    "def clean(text):\n",
    "    text = text.replace(\"\\t\", \" \").replace(\"\\r\", \"\")\n",
    "    text = re.sub(r\"\\[.*?\\]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "# ----------------------- Merge Loop -----------------------\n",
    "for name, subset in datasets_to_load:\n",
    "    try:\n",
    "        print(f\"\\nüì• Loading {name} ...\")\n",
    "        ds = load_dataset(name, subset, trust_remote_code=True)\n",
    "        count_before = len(dialogs)\n",
    "\n",
    "        for split in ds.keys():\n",
    "            for d in ds[split]:\n",
    "                text = None\n",
    "                if \"dialog\" in d:\n",
    "                    dialog_data = d[\"dialog\"]\n",
    "                    if isinstance(dialog_data, list):\n",
    "                        text = \"\\n\".join(dialog_data)\n",
    "                    elif isinstance(dialog_data, str):\n",
    "                        text = dialog_data\n",
    "\n",
    "                elif \"utterances\" in d:\n",
    "                    text = \"\\n\".join(\n",
    "                        u[\"text\"] for u in d[\"utterances\"] if \"text\" in u\n",
    "                    )\n",
    "\n",
    "                elif \"context\" in d and \"utterance\" in d:\n",
    "                    text = f\"{d['context']}\\n{d['utterance']}\"\n",
    "\n",
    "                elif \"text\" in d:\n",
    "                    text = d[\"text\"]\n",
    "\n",
    "                elif \"response\" in d:\n",
    "                    text = d[\"response\"]\n",
    "\n",
    "                if text and isinstance(text, str):\n",
    "                    t = clean(text)\n",
    "                    if len(t) > 10:\n",
    "                        dialogs.append(t)\n",
    "                        total_chars += len(t)\n",
    "\n",
    "        added = len(dialogs) - count_before\n",
    "        print(f\"‚úÖ {name}: added {added:,} dialogues, total ~{total_chars/1e6:.2f} MB\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Skipped {name} due to error:\\n{e}\\n\")\n",
    "\n",
    "# ----------------------- Save -----------------------\n",
    "merged_text = \"\\n\\n\".join(dialogs)\n",
    "output_path = \"data/human_chat_1gb.txt\"\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(merged_text)\n",
    "\n",
    "print(f\"\\n‚úÖ Saved merged dataset: {output_path}\")\n",
    "print(f\"üì¶ Final size: {len(merged_text.encode('utf-8'))/1e6:.2f} MB ({len(dialogs):,} dialogues)\")\n",
    "print(\"\\nüéâ Done! You can now train your Human Communication GPT.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "674fc68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì• Downloading yhavinga/dailydialog-parquet/train.parquet ...\n",
      "‚ö†Ô∏è Failed yhavinga/dailydialog-parquet: 404 Client Error. (Request ID: Root=1-69076e45-2a8e00714e3fd219406fcbc9;b2161432-631d-4775-aab6-5f350762f554)\n",
      "\n",
      "Repository Not Found for url: https://huggingface.co/yhavinga/dailydialog-parquet/resolve/main/train.parquet.\n",
      "Please make sure you specified the correct `repo_id` and `repo_type`.\n",
      "If you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication\n",
      "\n",
      "üì• Downloading facebook/empathetic_dialogues/train.json ...\n",
      "‚ö†Ô∏è Failed facebook/empathetic_dialogues: 404 Client Error. (Request ID: Root=1-69076e46-1c5fd6593149b5796c96567b;dcd56eb7-9940-49b7-b17b-5eda56b85cac)\n",
      "\n",
      "Repository Not Found for url: https://huggingface.co/facebook/empathetic_dialogues/resolve/main/train.json.\n",
      "Please make sure you specified the correct `repo_id` and `repo_type`.\n",
      "If you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication\n",
      "\n",
      "üì• Downloading lightchen/personachat_cleaned/train.parquet ...\n",
      "‚ö†Ô∏è Failed lightchen/personachat_cleaned: 404 Client Error. (Request ID: Root=1-69076e46-7cd606887bf3a4a23fa885e4;b9eb12b5-4d44-4794-a9ea-c4d562ef77a1)\n",
      "\n",
      "Repository Not Found for url: https://huggingface.co/lightchen/personachat_cleaned/resolve/main/train.parquet.\n",
      "Please make sure you specified the correct `repo_id` and `repo_type`.\n",
      "If you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication\n",
      "\n",
      "üì• Downloading Helsinki-NLP/OpenSubtitles/en_train.parquet ...\n",
      "‚ö†Ô∏è Failed Helsinki-NLP/OpenSubtitles: 404 Client Error. (Request ID: Root=1-69076e46-5cdcebc96074cc5845cfb241;c4330a0f-d7a0-4a5f-a420-ceb505a42d76)\n",
      "\n",
      "Repository Not Found for url: https://huggingface.co/Helsinki-NLP/OpenSubtitles/resolve/main/en_train.parquet.\n",
      "Please make sure you specified the correct `repo_id` and `repo_type`.\n",
      "If you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication\n",
      "‚ö†Ô∏è data/dailydialog-parquet/train.parquet not found, skipping\n",
      "‚ö†Ô∏è data/empathetic_dialogues/train.json not found, skipping\n",
      "‚ö†Ô∏è data/personachat_cleaned/train.parquet not found, skipping\n",
      "‚ö†Ô∏è data/opensubtitles/en_train.parquet not found, skipping\n",
      "\n",
      "‚úÖ Saved data/human_chat_1gb.txt  (0 dialogs, ~0.0 MB)\n",
      "\n",
      "üéâ Done ‚Äî you can now train your Human Communication GPT!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# üß† Build a 1 GB Human-Communication Dataset Automatically\n",
    "# ============================================================\n",
    "\n",
    "import os, re, shutil\n",
    "from huggingface_hub import login, hf_hub_download\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------- 1Ô∏è‚É£  LOGIN --------------------------\n",
    "# üëá Paste your HF token between the quotes ‚Üì‚Üì‚Üì‚Üì‚Üì\n",
    "HF_TOKEN = \"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\"\n",
    "login(token=HF_TOKEN)\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# ----------------------- 2Ô∏è‚É£  DOWNLOAD -----------------------\n",
    "downloads = [\n",
    "    (\"yhavinga/dailydialog-parquet\", \"train.parquet\", \"dailydialog-parquet/train.parquet\"),\n",
    "    (\"facebook/empathetic_dialogues\", \"train.json\", \"empathetic_dialogues/train.json\"),\n",
    "    (\"lightchen/personachat_cleaned\", \"train.parquet\", \"personachat_cleaned/train.parquet\"),\n",
    "    (\"Helsinki-NLP/OpenSubtitles\", \"en_train.parquet\", \"opensubtitles/en_train.parquet\"),\n",
    "]\n",
    "\n",
    "for repo, filename, outpath in downloads:\n",
    "    try:\n",
    "        print(f\"\\nüì• Downloading {repo}/{filename} ...\")\n",
    "        os.makedirs(os.path.dirname(f\"data/{outpath}\"), exist_ok=True)\n",
    "        local_path = hf_hub_download(repo_id=repo, filename=filename, token=HF_TOKEN)\n",
    "        shutil.copy(local_path, f\"data/{outpath}\")\n",
    "        print(f\"‚úÖ Saved to data/{outpath}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Failed {repo}: {e}\")\n",
    "\n",
    "# ----------------------- 3Ô∏è‚É£  MERGE --------------------------\n",
    "dialogs, total_chars = [], 0\n",
    "\n",
    "def clean(t):\n",
    "    t = re.sub(r\"\\s+\", \" \", str(t))\n",
    "    return t.strip()\n",
    "\n",
    "def add_texts(path, columns):\n",
    "    global total_chars\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"‚ö†Ô∏è {path} not found, skipping\")\n",
    "        return\n",
    "    print(f\"üìÇ Loading {path}\")\n",
    "    if path.endswith(\".parquet\"):\n",
    "        df = pd.read_parquet(path)\n",
    "    else:\n",
    "        df = pd.read_json(path, lines=True)\n",
    "    count = 0\n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            for t in df[col].dropna():\n",
    "                txt = clean(t)\n",
    "                if len(txt) > 10:\n",
    "                    dialogs.append(txt)\n",
    "                    total_chars += len(txt)\n",
    "                    count += 1\n",
    "    print(f\"‚úÖ Added {count:,} texts, total ~{total_chars/1e6:.1f} MB\")\n",
    "\n",
    "# Each dataset‚Äôs useful columns\n",
    "add_texts(\"data/dailydialog-parquet/train.parquet\", [\"dialog\"])\n",
    "add_texts(\"data/empathetic_dialogues/train.json\", [\"utterance\", \"context\"])\n",
    "add_texts(\"data/personachat_cleaned/train.parquet\", [\"text\"])\n",
    "add_texts(\"data/opensubtitles/en_train.parquet\", [\"translation\", \"text\"])\n",
    "\n",
    "# ----------------------- 4Ô∏è‚É£  SAVE ---------------------------\n",
    "merged = \"\\n\\n\".join(dialogs)\n",
    "out_path = \"data/human_chat_1gb.txt\"\n",
    "open(out_path, \"w\", encoding=\"utf-8\").write(merged)\n",
    "\n",
    "size_mb = len(merged.encode(\"utf-8\")) / 1e6\n",
    "print(f\"\\n‚úÖ Saved {out_path}  ({len(dialogs):,} dialogs, ~{size_mb:.1f} MB)\")\n",
    "print(\"\\nüéâ Done ‚Äî you can now train your Human Communication GPT!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41eb7ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì• Loading daily_dialog ...\n",
      "‚ö†Ô∏è  Skipped daily_dialog: Dataset scripts are no longer supported, but found daily_dialog.py\n",
      "\n",
      "üì• Loading empathetic_dialogues ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since empathetic_dialogues couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at C:\\Users\\aman\\.cache\\huggingface\\datasets\\empathetic_dialogues\\default\\0.1.0\\09bbeed3882a67db98c73952fb3c1c9a85af83dc78f81454c2454382fd03f6cf (last modified on Sat Nov  1 09:41:03 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ empathetic_dialogues: +14,426 samples, total ~0.2 MB\n",
      "\n",
      "üì• Loading blended_skill_talk ...\n",
      "‚úÖ blended_skill_talk: +4,507 samples, total ~0.3 MB\n",
      "\n",
      "üì• Loading bavard/personachat_truecased ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since bavard/personachat_truecased couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'full' at C:\\Users\\aman\\.cache\\huggingface\\datasets\\bavard___personachat_truecased\\full\\1.0.0\\73ee8f1a0d9e42255af5a8301877a2f3ac638e55b1cd9cbccca5ab7e23d2b638 (last modified on Sun Nov  2 18:16:30 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ bavard/personachat_truecased: +0 samples, total ~0.3 MB\n",
      "\n",
      "üì• Loading open_subtitles ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since open_subtitles couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'en-hi' at C:\\Users\\aman\\.cache\\huggingface\\datasets\\open_subtitles\\en-hi\\2018.0.0\\c1ec973ca4b6e588740d8f167cc0e24ea3f626e70bc7ffe467e944730500e198 (last modified on Sun Nov  2 18:23:30 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ open_subtitles: +0 samples, total ~0.3 MB\n",
      "\n",
      "üì• Loading multi_woz_v22 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since multi_woz_v22 couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'v2.2_active_only' at C:\\Users\\aman\\.cache\\huggingface\\datasets\\multi_woz_v22\\v2.2_active_only\\2.2.0\\6719c8b21478299411a0c6fdb7137c3ebab2e6425129af831687fb7851c69eb5 (last modified on Sun Nov  2 18:18:13 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ multi_woz_v22: +0 samples, total ~0.3 MB\n",
      "\n",
      "‚úÖ Saved data/human_chat_1gb.txt  (18,933 dialogs, ~0.3 MB)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Build a 1-GB Human-Communication Dataset (Current Hugging Face)\n",
    "# ============================================================\n",
    "\n",
    "from datasets import load_dataset\n",
    "import os, re\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "dialogs, total_chars = [], 0\n",
    "\n",
    "datasets_to_load = [\n",
    "    \"daily_dialog\",\n",
    "    \"empathetic_dialogues\",\n",
    "    \"blended_skill_talk\",\n",
    "    \"bavard/personachat_truecased\",\n",
    "    \"open_subtitles\",\n",
    "    \"multi_woz_v22\",\n",
    "]\n",
    "\n",
    "def clean(txt):\n",
    "    txt = re.sub(r\"\\s+\", \" \", str(txt))\n",
    "    return txt.strip()\n",
    "\n",
    "for name in datasets_to_load:\n",
    "    try:\n",
    "        print(f\"\\nüì• Loading {name} ...\")\n",
    "        ds = load_dataset(name)\n",
    "        count_before = len(dialogs)\n",
    "\n",
    "        for split in ds.keys():\n",
    "            for d in ds[split]:\n",
    "                text = None\n",
    "                for key in [\"dialog\", \"dialogue\", \"utterances\", \"text\",\n",
    "                            \"response\", \"context\", \"utterance\"]:\n",
    "                    if key in d:\n",
    "                        val = d[key]\n",
    "                        if isinstance(val, list):\n",
    "                            val = \"\\n\".join(map(str, val))\n",
    "                        text = str(val)\n",
    "                        break\n",
    "                if text and len(text) > 10:\n",
    "                    dialogs.append(clean(text))\n",
    "                    total_chars += len(text)\n",
    "\n",
    "        added = len(dialogs) - count_before\n",
    "        print(f\"‚úÖ {name}: +{added:,} samples, total ~{total_chars/1e6:.1f} MB\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Skipped {name}: {e}\")\n",
    "\n",
    "# Save merged text\n",
    "out_path = \"data/human_chat_1gb.txt\"\n",
    "open(out_path, \"w\", encoding=\"utf-8\").write(\"\\n\\n\".join(dialogs))\n",
    "size = len(open(out_path, \"rb\").read()) / 1e6\n",
    "print(f\"\\n‚úÖ Saved {out_path}  ({len(dialogs):,} dialogs, ~{size:.1f} MB)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "717c651e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ File found: data/dailydialog-parquet/train/dialogues_train.txt (6.04 MB)\n",
      "üìÑ Total lines: 11118\n",
      "üß© Sample content:\n",
      "Say , Jim , how about going for a few beers after dinner ? __eou__ You know that is tempting but is really not good for our fitness . __eou__ What do you mean ? It will help us to relax . __eou__ Do you really think so ? I don't . It will just make us fat and act silly . Remember last time ? __eou__ I guess you are right.But what shall we do ? I don't feel like sitting at home . __eou__ I suggest a walk over to the gym where we can play singsong and meet some of our friends . __eou__ That's a good idea . I hear Mary and Sally often go there to play pingpong.Perhaps we can make a foursome with them . __eou__ Sounds great to me ! If they are willing , we could ask them to go dancing with us.That is excellent exercise and fun , too . __eou__ Good.Let ' s go now . __eou__ All right . __eou__\n",
      "\n",
      "Can you do push-ups ? __eou__ Of course I can . It's a piece of cake ! Believe it or not , I can do 30 push-ups a minute . __eou__ Really ? I think that's impossible ! __eou__ You mean 30 push-ups ? __eou__ Yeah ! __eou__ It's easy . If you do exercise everyday , you can make it , too . __eou__\n",
      "\n",
      "Can you study with the radio on ? __eou__ No , I listen to background music . __eou__ What is the difference ? __eou__ The radio has too many comerials . __eou__ That's true , but then you have to buy a record player . __eou__\n",
      "\n",
      "Are you all right ? __eou__ I will be all right soon . I was terrified when I watched them fall from the wire . __eou__ Don't worry.He is an acrobat „ÄÇ __eou__ I see . __eou__\n",
      "\n",
      "Hey John , nice skates . Are they new ? __eou__ Yeah , I just got them . I started playing ice hockey in a community league . So , I finally got myself new skates . __eou__ What position do you play ? __eou__ I ‚Äô m a defender . It ‚Äô s a lot of fun . You don ‚Äô t have to be able to skate as fast on defense . __eou__ Yeah , you ‚Äô re a pretty big guy . I play goalie , myself . __eou__ Oh , yeah ? Which team ? __eou__ The Rockets . __eou__ Really ? I think we play you guys next week . Well , I have to go to practice . See you later . __eou__ All right , see you later . __eou__\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = \"data/dailydialog-parquet/train/dialogues_train.txt\"\n",
    "\n",
    "if os.path.exists(path):\n",
    "    size_mb = os.path.getsize(path) / 1e6\n",
    "    print(f\"‚úÖ File found: {path} ({size_mb:.2f} MB)\")\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        print(f\"üìÑ Total lines: {len(lines)}\")\n",
    "        print(\"üß© Sample content:\")\n",
    "        print(\"\\n\".join(lines[:5]))\n",
    "else:\n",
    "    print(\"‚ùå File missing ‚Äî check your download path.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee8d1d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
